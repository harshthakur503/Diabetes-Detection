{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None) # Show all columns\n",
    "pd.set_option('display.max_rows', None) # Show all rows\n",
    "pd.set_option('display.expand_frame_repr', False) # Avoid wrapping the output to multiple lines\n",
    "\n",
    "import openpyxl\n",
    "import numpy as np\n",
    "import io\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StringIO object\n",
    "output_buffer = io.StringIO()\n",
    "\n",
    "# Redirect stdout to the StringIO object\n",
    "sys.stdout = output_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the Excel file \n",
    "excel_file_path = r\"C:\\Users\\hthakur2\\OneDrive - Teck Resources Limited\\Documents\\Power BI- UAT Automation\\Testing Status.xlsx\"\n",
    "page_name = 'Testing Tracker'\n",
    "excel_file = pd.read_excel(excel_file_path, sheet_name=page_name, header = 1)\n",
    "\n",
    "filter_release_name = excel_file.loc[excel_file['Release Name'] == \"D1\"]\n",
    "print(\"Values in Release D1\")\n",
    "#null_column_name = 'Release Planned Date'\n",
    "filter_release_name['Release Planned Date '] = filter_release_name['Release Planned Date'].replace({pd.NaT: np.nan, np.nan: ''})\n",
    "#filter_release_name[null_column_name] = np.where(pd.isnull(filter_release_name[null_column_name]), pd.NaT, filter_release_name[null_column_name].fillna(''))\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "print(filter_release_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the CSV files into pandas dataframes\n",
    "file1 = r\"C:\\Users\\hthakur2\\OneDrive - Teck Resources Limited\\Documents\\Power BI- UAT Automation\\powerbi_file1.csv\"\n",
    "file2a = r\"C:\\Users\\hthakur2\\OneDrive - Teck Resources Limited\\Documents\\Power BI- UAT Automation\\powerbi_file2.csv\"\n",
    "file2b = r\"C:\\Users\\hthakur2\\OneDrive - Teck Resources Limited\\Documents\\Power BI- UAT Automation\\powerbi_file2b.csv\"\n",
    "file3a = r\"C:\\Users\\hthakur2\\OneDrive - Teck Resources Limited\\Documents\\Power BI- UAT Automation\\powerbi_file3.csv\"\n",
    "file3b = r\"C:\\Users\\hthakur2\\OneDrive - Teck Resources Limited\\Documents\\Power BI- UAT Automation\\powerbi_file3b.csv\"\n",
    "file4a = r\"C:\\Users\\hthakur2\\OneDrive - Teck Resources Limited\\Documents\\Power BI- UAT Automation\\powerbi_file4a.csv\"\n",
    "file4b = r\"C:\\Users\\hthakur2\\OneDrive - Teck Resources Limited\\Documents\\Power BI- UAT Automation\\powerbi_file4b.csv\"\n",
    "file4c = r\"C:\\Users\\hthakur2\\OneDrive - Teck Resources Limited\\Documents\\Power BI- UAT Automation\\powerbi_file4c.csv\"\n",
    "file5a = r\"C:\\Users\\hthakur2\\OneDrive - Teck Resources Limited\\Documents\\Power BI- UAT Automation\\powerbi_file5a.csv\"\n",
    "file5b = r\"C:\\Users\\hthakur2\\OneDrive - Teck Resources Limited\\Documents\\Power BI- UAT Automation\\powerbi_file5b.csv\"\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "dataframe1 = pd.read_csv(file1) #D1-DM\n",
    "dataframes.append(dataframe1)\n",
    "\n",
    "dataframe2a = pd.read_csv(file2a) #D2 HVC WO\n",
    "dataframe2b = pd.read_csv(file2b) #D2 CDA Plant \n",
    "dataframes.append(dataframe2a)\n",
    "dataframes.append(dataframe2b)\n",
    "\n",
    "dataframe3a = pd.read_csv(file3a) #D3 RDM FMS\n",
    "dataframe3b = pd.read_csv(file3b) #D3 RDM Plant\n",
    "dataframes.append(dataframe3a)\n",
    "dataframes.append(dataframe3b)\n",
    "\n",
    "dataframe4a = pd.read_csv(file4a)\n",
    "dataframe4b = pd.read_csv(file4b)\n",
    "dataframe4c = pd.read_csv(file4c)\n",
    "dataframes.append(dataframe4a)\n",
    "dataframes.append(dataframe4b)\n",
    "dataframes.append(dataframe4c)\n",
    "\n",
    "\n",
    "dataframe5a = pd.read_csv(file5a)\n",
    "dataframe5b = pd.read_csv(file5b)\n",
    "dataframes.append(dataframe5a)\n",
    "dataframes.append(dataframe5b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to perform data validations\n",
    "def check_for_nan_values(dataframe):\n",
    "    nan_values = dataframe[dataframe.isna().any(axis=1)].fillna('')\n",
    "    if len(nan_values) > 0:\n",
    "        print(f\"\\n{len(nan_values)} NaN values found in the following rows:\")\n",
    "        print(\"\\n\") \n",
    "        print(nan_values)\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(\"No NaN values found.\")\n",
    "\n",
    "\n",
    "\n",
    "# This function checks for NaN (Not a Number) values in the dataframe\n",
    "\n",
    "\n",
    "#This function checks for negative values in numeric columns of the dataframe\n",
    "def check_for_negative_values(dataframe):\n",
    "    # Select only numeric columns\n",
    "    numeric_cols = dataframe.select_dtypes(include=['float', 'int']).columns\n",
    "    \n",
    "    # Check for negative values\n",
    "    negative_values = dataframe[numeric_cols].lt(0).sum()\n",
    "    \n",
    "    if negative_values.sum() > 0:\n",
    "        print(f\"\\n{negative_values.sum()} Negative values found in the following rows:\")\n",
    "        print(\"\\n\")\n",
    "        print(dataframe[dataframe[numeric_cols].lt(0).any(axis=1)])\n",
    "        print(\"\\n\")\n",
    "\n",
    "    else:\n",
    "        print(\"No negative values found\")\n",
    "        print(\"\\n\")\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "def check_for_values_above_100(dataframe):\n",
    "    # Get percentage columns\n",
    "    percentage_cols = [col for col in dataframe.columns if \"%\" in col]\n",
    "    if not percentage_cols:\n",
    "        print(\"No percentage columns found. No value is above 100%\")\n",
    "        return None\n",
    "    # Remove percent sign and convert to numeric\n",
    "    percentage_cols_numeric = dataframe[percentage_cols].replace('%', '', regex=True).apply(pd.to_numeric, errors='coerce')\n",
    "    # Check for values above 100%\n",
    "    values_above_100 = percentage_cols_numeric[percentage_cols_numeric > 100]\n",
    "    if values_above_100.isnull().values.any():\n",
    "        values_above_100 = values_above_100.dropna(how='all')\n",
    "    if not values_above_100.empty:\n",
    "        print(f\"\\n{len(values_above_100)} values above 100% found in the following rows:\")\n",
    "        print(\"\\n\") \n",
    "        print(dataframe[dataframe.index.isin(values_above_100.index)])\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(\"No values above 100% found.\")\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def check_consecutive_values(df):\n",
    "    date_range_col = next((col for col in ['Planning Week Date Range (Mon - Sun)', 'Month-Year'] if col in df.columns), None)\n",
    "    if not date_range_col:\n",
    "        print('Neither Planning Week Date Range nor Month-Year column found')\n",
    "        return\n",
    "    duplicates_found = False\n",
    "    for col in df.columns:\n",
    "        if col not in ['Planning Week Date Range (Mon - Sun)', 'Mine/Mill', 'Site Code']:\n",
    "            # Find consecutive values that repeat at least three times\n",
    "            for site_code, site_df in df.groupby('Site Code'):\n",
    "                site_df = site_df.sort_values(by=[date_range_col])\n",
    "                consecutive_count = 1\n",
    "                last_value = None\n",
    "                start_index = None\n",
    "                for idx, row in site_df.iterrows():\n",
    "                    value = row[col]\n",
    "                    if value == last_value:\n",
    "                        consecutive_count += 1\n",
    "                        if consecutive_count == 3:\n",
    "                            start_index = idx - 1\n",
    "                    else:\n",
    "                        consecutive_count = 1\n",
    "                        last_value = value\n",
    "                        start_index = None\n",
    "                    if start_index is not None:\n",
    "                        end_index = idx\n",
    "                        date_range = row[date_range_col]\n",
    "                        duplicates_found = True\n",
    "                        if duplicates_found:\n",
    "                            print('\\nDuplicates found:')\n",
    "                        print(f\"Site Code: {site_code}, Column: {col}, Value: {value}, {date_range_col}: {date_range}\")\n",
    "    if not duplicates_found:\n",
    "        print('No duplicate values found')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(dataframes):\n",
    "    print(\"\\n\\n\" + f\"CHECKS FOR FILE {i+1}...\\n\")\n",
    "    \n",
    "    check_for_nan_values(df)\n",
    "    check_for_negative_values(df)\n",
    "    check_for_values_above_100(df)\n",
    "    check_consecutive_values(df)\n",
    "   \n",
    "    \n",
    "sys.stdout = sys.__stdout__\n",
    "output = output_buffer.getvalue()\n",
    "with open(\"output.txt\", \"w\") as f:\n",
    "    # Write the output to the file\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54d1709daba39fa36aefb32b61e909af976a6526b2b3e679262a3bb9423f8a4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
